{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCEv+i1aPy1zuE9glB+9AW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishal-113/NLP-2/blob/main/Bigram_Probabilities_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "From the table (sums shown where needed):\n",
        "\n",
        "* For `<s>`: counts → I:2, deep:1 → total $=2+1=3$.\n",
        "  So $P(I\\mid \\<s\\>)=2/3$, $P(deep\\mid \\<s\\>)=1/3$.\n",
        "\n",
        "* For `I`: love:2 → total $=2$. So $P(love\\mid I)=2/2=1.$\n",
        "\n",
        "* For `love`: NLP:1, deep:1 → total $=1+1=2$. So $P(NLP\\mid love)=1/2$, $P(deep\\mid love)=1/2$.\n",
        "\n",
        "* For `NLP`: </s>:1 → total $=1$. So $P(</s>\\mid NLP)=1.$\n",
        "\n",
        "* For `deep`: learning:2 → total $=2$. So $P(learning\\mid deep)=2/2=1.$\n",
        "\n",
        "* For `learning`: </s>:1, is:1 → total $=1+1=2$. So $P(</s>\\mid learning)=1/2$.\n",
        "\n",
        "\n",
        "### Sentence S1: `<s> I love NLP </s>`\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "P(S1) &= P(I\\mid\\<s\\>) \\times P(love\\mid I) \\times P(NLP\\mid love) \\times P(</s>\\mid NLP)\\\\[6pt]\n",
        "&= \\frac{2}{3}\\times 1 \\times \\frac{1}{2}\\times 1.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Multiply step-by-step (digit-check):\n",
        "\n",
        "* $\\frac{2}{3}\\times\\frac{1}{2}=\\frac{2}{6}=\\frac{1}{3}.$ (units: $2\\times1=2$; denominators: $3\\times2=6$; reduce $2/6=1/3$.)\n",
        "\n",
        "So\n",
        "\n",
        "$$\n",
        "\\boxed{P(S1)=\\tfrac{1}{3}\\approx 0.333333.}\n",
        "$$\n",
        "\n",
        "\n",
        "### Sentence S2: `<s> I love deep learning </s>`\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "P(S2) &= P(I\\mid\\<s\\>) \\times P(love\\mid I) \\times P(deep\\mid love)\\times P(learning\\mid deep)\\times P(</s>\\mid learning)\\\\[6pt]\n",
        "&= \\frac{2}{3}\\times 1 \\times \\frac{1}{2}\\times 1 \\times \\frac{1}{2}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Combine the non-1 factors:\n",
        "\n",
        "* $\\frac{2}{3}\\times\\frac{1}{2}\\times\\frac{1}{2}=\\frac{2}{3}\\times\\frac{1}{4}=\\frac{2}{12}=\\frac{1}{6}.$ (digit-check: $2\\times1\\times1=2$ numerator; $3\\times2\\times2=12$ denominator; reduce $2/12=1/6$.)\n",
        "\n",
        "So\n",
        "\n",
        "$$\n",
        "\\boxed{P(S2)=\\tfrac{1}{6}\\approx 0.166667.}\n",
        "$$\n",
        "\n",
        "\n",
        "### Which is more probable?\n",
        "\n",
        "$P(S1)=1/3$ vs $P(S2)=1/6$.\n",
        "$\\boxed{\\text{S1 is more probable under the bigram model.}}$\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Zero-probability problem\n",
        "\n",
        "### a) $P(\\text{noodle}\\mid ate)$ under MLE\n",
        "\n",
        "From the table, `ate` has counts: lunch:6, dinner:3, a:2, the:1 → total $=6+3+2+1=12$. There is **no** count for `noodle` after `ate`, so\n",
        "\n",
        "$$\n",
        "\\boxed{P(\\text{noodle}\\mid ate)_{\\text{MLE}} = \\frac{0}{12} = 0.}\n",
        "$$\n",
        "\n",
        "### b) Why this is a problem\n",
        "\n",
        "Because sentence probabilities are products of conditional probabilities, a single zero bigram probability makes the entire sentence probability zero. That:\n",
        "\n",
        "* **Kills the model’s ability to score or compare sentences** containing unseen bigrams (gives them probability 0).\n",
        "* **Breaks perplexity calculations** (log of zero is $-\\infty$), so perplexity becomes undefined or infinite.\n",
        "* In short: MLE **doesn’t generalize** to unseen events — it assigns zero probability instead of a small plausible value.\n",
        "\n",
        "### c) Laplace (Add-1) smoothing for $P(\\text{noodle}\\mid ate)$\n",
        "\n",
        "Add-1 formula:\n",
        "\n",
        "$$\n",
        "P_{\\text{Laplace}}(w\\mid ate)=\\frac{\\text{count}(ate,w)+1}{\\text{total\\_after\\_ate} + V}.\n",
        "$$\n",
        "\n",
        "Given: total after `ate` = 12, vocabulary size $V=10$.\n",
        "\n",
        "So\n",
        "\n",
        "$$\n",
        "P_{\\text{Laplace}}(\\text{noodle}\\mid ate)=\\frac{0+1}{12+10}=\\frac{1}{22}.\n",
        "$$\n",
        "\n",
        "Digit-check: $12+10=22$. So\n",
        "\n",
        "$$\n",
        "\\boxed{P_{\\text{Laplace}}(\\text{noodle}\\mid ate)=\\tfrac{1}{22}\\approx 0.0454545.}\n",
        "$$\n",
        "\n",
        "\n",
        "## Short summary (answers only)\n",
        "\n",
        "* $P(S1)=\\tfrac{1}{3}\\approx0.3333.$\n",
        "* $P(S2)=\\tfrac{1}{6}\\approx0.1667.$\n",
        "* S1 is more probable.\n",
        "* $P(\\text{noodle}\\mid ate)_{\\text{MLE}}=0$ — zero causes whole-sentence probability to be zero and breaks perplexity.\n",
        "* With Add-1 (V=10, total after `ate`=12): $P(\\text{noodle}\\mid ate)=\\tfrac{1}{22}\\approx0.04545.$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YphOT9s_UTwM"
      }
    }
  ]
}