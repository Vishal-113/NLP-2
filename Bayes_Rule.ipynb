{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOVwP6eygzZZJksIiirbEL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishal-113/NLP-2/blob/main/Bayes_Rule.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "### **1. Meaning of each term**\n",
        "\n",
        "* **$P(c)$ – Prior probability of the class**\n",
        "  How likely class $c$ is *before* seeing the document.\n",
        "  Example: If 40% of emails are spam, then $P(\\text{spam}) = 0.4$.\n",
        "\n",
        "* **$P(d \\mid c)$ – Likelihood of the document given the class**\n",
        "  The probability of observing document $d$ *if we assume it belongs to class $c$*.\n",
        "  Example: If the word \"lottery\" appears often in spam, then $P(\\text{document contains “lottery”} \\mid \\text{spam})$ is high.\n",
        "\n",
        "* **$P(c \\mid d)$ – Posterior probability of the class given the document**\n",
        "  What we ultimately want: the probability that the document belongs to class $c$ *after seeing its contents*.\n",
        "  Bayes’ Rule tells us:\n",
        "\n",
        "  $$\n",
        "  P(c \\mid d) = \\frac{P(d \\mid c) P(c)}{P(d)}\n",
        "  $$\n",
        "\n",
        "\n",
        "\n",
        "### **2. Why can the denominator $P(d)$ be ignored?**\n",
        "\n",
        "* In Bayes’ Rule,\n",
        "\n",
        "  $$\n",
        "  P(c \\mid d) = \\frac{P(d \\mid c) P(c)}{P(d)}\n",
        "  $$\n",
        "\n",
        "  the denominator $P(d)$ = probability of the document itself (regardless of class).\n",
        "\n",
        "* When comparing across classes (e.g., spam vs. ham), the document $d$ is fixed — so $P(d)$ is **the same for all classes**.\n",
        "\n",
        "* Since we only care about finding the class with the highest probability, $P(d)$ doesn’t affect the comparison.\n",
        "\n",
        "* That’s why the decision rule simplifies to:\n",
        "\n",
        "  $$\n",
        "  c_{MAP} = \\arg\\max_{c \\in C} P(c) \\, P(d \\mid c)\n",
        "  $$\n",
        "\n",
        "\n",
        "\n",
        " So in words: *We choose the class that maximizes the product of the prior $P(c)$ and the likelihood $P(d \\mid c)$, because the denominator $P(d)$ is constant and cancels out in the comparison.*\n",
        "\n"
      ],
      "metadata": {
        "id": "jWHKTnO01VXU"
      }
    }
  ]
}