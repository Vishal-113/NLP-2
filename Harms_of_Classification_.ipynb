{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPl6USzK4safZmuVl8NqkTX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishal-113/NLP-2/blob/main/Harms_of_Classification_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **1. Representational harm**\n",
        "\n",
        "* **Definition:** Representational harm happens when a system perpetuates or amplifies stereotypes, marginalizes certain groups, or portrays them unfairly — even if the system is not directly denying resources or access. It’s about how people or groups are *depicted* or *framed* by the classifier.\n",
        "* **Kiritchenko & Mohammad (2018):** In their study on sentiment and emotion in text, they found that sentiment analysis systems often misclassified words related to certain social groups as more negative or more emotionally charged. For example, identity-related terms (like “gay” or “Muslim”) were more likely to be tagged with negative sentiment. This demonstrates **representational harm**, since the model reflects and reinforces social stereotypes in how groups are represented.\n",
        "\n",
        "\n",
        "### **2. Risk of censorship in toxicity classification**\n",
        "\n",
        "* **From Dixon et al. (2018), Oliva et al. (2021):** A major risk is that toxicity classifiers may **over-block legitimate speech** — particularly when they mistake mentions of marginalized identities for toxicity. For instance, a system might flag posts where people talk about their own identities (e.g., “I’m a gay man”) as toxic, leading to silencing of marginalized voices. This creates **censorship bias**, where communities most needing safe expression get unfairly censored.\n",
        "\n",
        "### **3. Why classifiers may perform worse on African American English (AAE) or Indian English**\n",
        "\n",
        "* Many classifiers are trained primarily on **Standard American English** or other dominant varieties.\n",
        "* As a result:\n",
        "\n",
        "  * **Vocabulary, grammar, and spelling differences** in AAE or Indian English may not be well-represented in training data.\n",
        "  * **Bias in annotation:** human annotators may misinterpret these varieties as more emotional, informal, or even toxic.\n",
        "  * **Data imbalance:** underrepresentation in training datasets means the model has less exposure to these forms.\n",
        "* Altogether, this leads to systematically **worse accuracy and higher error rates** for these English varieties, even though they are fully legitimate.\n",
        "\n",
        "\n",
        " So in short:\n",
        "\n",
        "1. Representational harm = unfair portrayal (Kiritchenko & Mohammad showed bias against identity terms).\n",
        "2. Risk of censorship = silencing marginalized speech due to over-flagging.\n",
        "3. Poorer performance on AAE/Indian English = underrepresentation, annotation bias, and mismatch with training data.\n",
        "\n"
      ],
      "metadata": {
        "id": "LhTeF_MWQo2y"
      }
    }
  ]
}