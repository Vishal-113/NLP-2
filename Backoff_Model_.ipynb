{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8c0nihxOfA77QxUlWctyY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishal-113/NLP-2/blob/main/Backoff_Model_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Sentences:\n",
        "\n",
        "* `<s> I like cats </s>`\n",
        "* `<s> I like dogs </s>`\n",
        "* `<s> You like cats </s>`\n",
        "\n",
        "From these:\n",
        "\n",
        "* $C(I,\\,like)=2$ (two sentences start `I like ...`)\n",
        "* $C(You,\\,like)=1$\n",
        "* $C(I,\\,like,\\,cats)=1$\n",
        "* $C(I,\\,like,\\,dogs)=1$\n",
        "* $C(You,\\,like,\\,cats)=1$\n",
        "* $C(like,\\,cats)=2$\n",
        "* $C(like,\\,dogs)=1$\n",
        "* Total continuations after `like`: $C'(like)=2+1=3.$\n",
        "\n",
        "\n",
        "\n",
        "## 1) $P(\\text{cats}\\mid I,\\,like)$ — trigram MLE\n",
        "\n",
        "Use\n",
        "\n",
        "$$\n",
        "P(w_3\\mid w_1,w_2)=\\frac{C(w_1,w_2,w_3)}{C(w_1,w_2)}.\n",
        "$$\n",
        "\n",
        "Here $C(I,like,cats)=1$ and $C(I,like)=2$. So\n",
        "\n",
        "$$\n",
        "P(\\text{cats}\\mid I,like)=\\frac{1}{2}=0.5.\n",
        "$$\n",
        "\n",
        "(Digit check: numerator $1$, denominator $2$ → $1/2=0.5$.)\n",
        "\n",
        "\n",
        "## 2) $P(\\text{dogs}\\mid You,\\,like)$ — trigram → bigram backoff\n",
        "\n",
        "First try trigram MLE:\n",
        "\n",
        "* $C(You,like,dogs)=0$ (no `You like dogs` in corpus) → trigram MLE = $0/ C(You,like)=0$.\n",
        "  Because the trigram is unseen we **back off** to the bigram `like`.\n",
        "\n",
        "Bigram MLE:\n",
        "\n",
        "$$\n",
        "P(\\text{dogs}\\mid like)=\\frac{C(like,dogs)}{\\sum_x C(like,x)}=\\frac{1}{3}.\n",
        "$$\n",
        "\n",
        "So with trigram→bigram backoff we assign\n",
        "\n",
        "$$\n",
        "\\boxed{P(\\text{dogs}\\mid You,like)=\\tfrac{1}{3}\\approx 0.3333.}\n",
        "$$\n",
        "\n",
        "(Digit check: $C(like,dogs)=1$, total after `like` = $2+1=3$ → $1/3$.)\n",
        "\n",
        "\n",
        "\n",
        "## 3) Why backoff is necessary (short)\n",
        "\n",
        "* The training data is tiny, so many possible trigrams never appear. Trigram MLE would give **zero probability** to any unseen trigram (like `You like dogs`), which would make any sentence containing that trigram impossible under the model.\n",
        "* **Backoff** lets us use a lower-order model (bigram, unigram) when higher-order counts are zero — this provides non-zero, more robust estimates and better generalization from sparse data.\n",
        "\n"
      ],
      "metadata": {
        "id": "ErR55QosVdBR"
      }
    }
  ]
}